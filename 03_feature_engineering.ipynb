{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:23:28.501881Z",
     "start_time": "2025-12-03T06:23:28.499731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ],
   "id": "93ab35f2fe839133",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:23:28.981926Z",
     "start_time": "2025-12-03T06:23:28.980438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# paths\n",
    "merged_path = \"data/processed/train_merged.parquet\"\n",
    "out_feat_path = \"data/processed/train_features_day3_recomputed.parquet\""
   ],
   "id": "28e36ad12c8d37e1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:23:29.374386Z",
     "start_time": "2025-12-03T06:23:29.370753Z"
    }
   },
   "cell_type": "code",
   "source": "assert os.path.exists(out_feat_path), f\"{out_feat_path} does not exist\"",
   "id": "8755aa8fffb5a7e3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:23:30.184424Z",
     "start_time": "2025-12-03T06:23:29.590672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load the full merged dataset\n",
    "\n",
    "full  = pd.read_parquet(merged_path)\n",
    "print(\"Loaded full : \", full.shape)"
   ],
   "id": "a3c97c691d84a354",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded full :  (590540, 435)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:23:30.228055Z",
     "start_time": "2025-12-03T06:23:30.225935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# creation of 'dt' incase if it missing\n",
    "\n",
    "if 'dt' not in full.columns:\n",
    "    if 'TransactionDT' in full.columns:\n",
    "        START_DATE = \"2017-12-01\"\n",
    "        full['dt'] = pd.to_datetime(full['TransactionDT'], unit='s', origin=START_DATE)\n",
    "    elif 'timestamp' in full.columns:\n",
    "        full['dt'] = pd.to_datetime(full['timestamp'])\n",
    "    else:\n",
    "        raise RuntimeError(\"No dt/TransactionDT/timestamp in merged file.\")"
   ],
   "id": "16c90ab617c569e1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:23:30.241134Z",
     "start_time": "2025-12-03T06:23:30.237717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ensure TransactionAmt exists and numeric\n",
    "full['TransactionAmt'] = full['TransactionAmt'].astype(float)"
   ],
   "id": "831ca4f7098b77aa",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:23:30.246828Z",
     "start_time": "2025-12-03T06:23:30.245167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# helper function\n",
    "\n",
    "def rolling_count_seconds(ts_arr, window_seconds):\n",
    "    left_idx = np.searchsorted(ts_arr, ts_arr - window_seconds, side='left')\n",
    "    return (np.arange(len(ts_arr)) - left_idx + 1).astype(np.int32)\n",
    "\n",
    "def rolling_sum_seconds(ts_arr, val_arr, window_seconds):\n",
    "    left_idx = np.searchsorted(ts_arr, ts_arr - window_seconds, side='left')\n",
    "    csum = np.cumsum(val_arr)\n",
    "    prev = np.where(left_idx > 0, csum[left_idx - 1], 0.0)\n",
    "    return (csum - prev).astype(float)"
   ],
   "id": "89171a67c3898203",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:23:31.210087Z",
     "start_time": "2025-12-03T06:23:30.394687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prepare seconds column\n",
    "full = full.sort_values(['card1', 'dt']).reset_index(drop=True)\n",
    "full['ts'] = (full['dt'].astype('int64') // 10*9).astype(np.int64)"
   ],
   "id": "1fafe74f2f64deb8",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:23:32.346696Z",
     "start_time": "2025-12-03T06:23:32.344162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# windows\n",
    "\n",
    "w_1d = 24*3600\n",
    "w_7d = 7*24*3600\n",
    "w_5m = 5*60\n",
    "w_30m = 30*60"
   ],
   "id": "b69ba40d0179e432",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:23:34.463672Z",
     "start_time": "2025-12-03T06:23:32.648275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# compute card1-level velocity & sums — group_keys=False keeps original index alignment\n",
    "\n",
    "full['card1_txt_count_1d'] = full.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, w_1d), index=g.index)\n",
    ")\n",
    "full['card1_txt_count_7d'] = full.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, w_7d), index=g.index)\n",
    ")"
   ],
   "id": "99096fd620d4a098",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_29178/2629617809.py:3: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  full['card1_txt_count_1d'] = full.groupby('card1', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_29178/2629617809.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  full['card1_txt_count_7d'] = full.groupby('card1', group_keys=False).apply(\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:23:36.233040Z",
     "start_time": "2025-12-03T06:23:34.467123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "full['card1_txt_5min'] = full.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, w_5m), index=g.index)\n",
    ")\n",
    "full['card1_txt_30min'] = full.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, w_30m), index=g.index)\n",
    ")"
   ],
   "id": "4ba119dd6f43d6fa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_29178/650588435.py:1: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  full['card1_txt_5min'] = full.groupby('card1', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_29178/650588435.py:4: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  full['card1_txt_30min'] = full.groupby('card1', group_keys=False).apply(\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:23:38.229136Z",
     "start_time": "2025-12-03T06:23:36.237410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "full['card1_amt_sum_1d'] = full.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_sum_seconds(g['ts'].values, g['TransactionAmt'].values, w_1d), index=g.index)\n",
    ")\n",
    "full['card1_amt_sum_7d'] = full.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_sum_seconds(g['ts'].values, g['TransactionAmt'].values, w_7d), index=g.index)\n",
    ")\n"
   ],
   "id": "1a011bb889788a70",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_29178/2513322730.py:1: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  full['card1_amt_sum_1d'] = full.groupby('card1', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_29178/2513322730.py:4: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  full['card1_amt_sum_7d'] = full.groupby('card1', group_keys=False).apply(\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:24:00.936934Z",
     "start_time": "2025-12-03T06:24:00.835373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# expanding mean and deviation per card1 (expanding keeps one-to-one alignment)\n",
    "\n",
    "full['card1_amt_mean'] = full.groupby('card1')['TransactionAmt'].expanding().mean().reset_index(level=0, drop=True)\n",
    "full['card1_amt_dev'] = full['TransactionAmt'] - full['card1_amt_mean']"
   ],
   "id": "b8a92d8b6d75e646",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:24:08.163634Z",
     "start_time": "2025-12-03T06:24:01.735798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# user_region features\n",
    "\n",
    "full['user_region'] = full['card1'].astype(str)+'_'+full['addr1'].astype(str)\n",
    "full = full.sort_values(['user_region','dt']).reset_index(drop=True)\n",
    "full['ts'] = (full['dt'].astype('int64') // 10*9).astype(np.int64)\n",
    "full['region_text_count_7d'] = full.groupby('user_region', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, w_7d), index=g.index)\n",
    ")\n",
    "full['region_amt_sum_7d'] = full.groupby('user_region', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_sum_seconds(g['ts'].values, g['TransactionAmt'].values, w_7d), index=g.index)\n",
    ")\n",
    "full['region_amt_mean'] = full.groupby('user_region')['TransactionAmt'].expanding().mean().reset_index(level=0, drop=True)"
   ],
   "id": "b88b5df68778c836",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_29178/3754893501.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  full['region_text_count_7d'] = full.groupby('user_region', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_29178/3754893501.py:9: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  full['region_amt_sum_7d'] = full.groupby('user_region', group_keys=False).apply(\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:24:10.839951Z",
     "start_time": "2025-12-03T06:24:09.696494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# device features: sort by DeviceInfo\n",
    "\n",
    "full = full.sort_values(['DeviceInfo','dt']).reset_index(drop=True)\n",
    "full['ts'] = (full['dt'].astype('int64') // 10*9).astype(np.int64)\n",
    "full['device_text_count_7d'] = full.groupby('DeviceInfo', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, w_7d), index=g.index)\n",
    ")\n",
    "full['device_amt_sum_7d'] = full.groupby('DeviceInfo', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_sum_seconds(g['ts'].values, g['TransactionAmt'].values, w_7d), index=g.index)\n",
    ")\n",
    "full['device_amt_mean'] = full.groupby('DeviceInfo')['TransactionAmt'].expanding().mean().reset_index(level=0, drop=True)"
   ],
   "id": "5fd513cb649dc7b4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_29178/2193520672.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  full['device_text_count_7d'] = full.groupby('DeviceInfo', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_29178/2193520672.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  full['device_text_count_7d'] = full.groupby('DeviceInfo', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_29178/2193520672.py:8: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  full['device_amt_sum_7d'] = full.groupby('DeviceInfo', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_29178/2193520672.py:8: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  full['device_amt_sum_7d'] = full.groupby('DeviceInfo', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_29178/2193520672.py:11: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  full['device_amt_mean'] = full.groupby('DeviceInfo')['TransactionAmt'].expanding().mean().reset_index(level=0, drop=True)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:24:11.354545Z",
     "start_time": "2025-12-03T06:24:10.842850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sequence features: time since prev, amount diff/ratio\n",
    "\n",
    "full = full.sort_values(['card1','dt']).reset_index(drop=True)\n",
    "full['dt_prev'] = full.groupby('card1')['dt'].shift(1)\n",
    "full['time_since_prev'] = (full['dt'] - full['dt_prev']).dt.total_seconds().fillna(0)\n",
    "full['amt_prev'] = full.groupby('card1')['TransactionAmt'].shift(1).fillna(0)\n",
    "full['amt_diff_prev'] = full['TransactionAmt'] - full['amt_prev']\n",
    "full['amt_ratio_prev'] = full['TransactionAmt'] / (full['amt_prev'] + 1)"
   ],
   "id": "1e2bece50c1d1d98",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:24:11.781302Z",
     "start_time": "2025-12-03T06:24:11.359746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# final: restore time order and save only feature cols + isFraud + dt\n",
    "full = full.sort_values('dt').reset_index(drop=True)\n",
    "save_cols = [\n",
    "    'card1_txn_count_1d','card1_txn_count_7d','card1_amt_sum_1d','card1_amt_mean','card1_amt_dev',\n",
    "    'region_txn_count_7d','region_amt_mean','device_txn_count_7d','device_amt_mean',\n",
    "    'time_since_prev','amt_diff_prev','amt_ratio_prev','card1_txn_5min','card1_txn_30min',\n",
    "    'isFraud','dt'\n",
    "]"
   ],
   "id": "d9611ad72f18686f",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:24:12.613103Z",
     "start_time": "2025-12-03T06:24:12.494884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# keep only those that exist to avoid errors\n",
    "save_cols = [c for c in save_cols if c in full.columns]\n",
    "full[save_cols].to_parquet(out_feat_path, index=False)\n",
    "\n",
    "print(\"Recomputed & saved features rows:\", len(full), \"cols:\", len(save_cols))\n",
    "print(\"Saved to:\", out_feat_path)"
   ],
   "id": "7bf230d4e2827698",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomputed & saved features rows: 590540 cols: 10\n",
      "Saved to: data/processed/train_features_day3_recomputed.parquet\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T06:24:14.204612Z",
     "start_time": "2025-12-03T06:24:14.202416Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "716439de1c97b276",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "564e42fe54c536f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# paths\n",
    "merged_path = \"data/processed/train_merged.parquet\"\n",
    "out_feat_path = \"data/processed/train_features_day3_recomputed.parquet\"\n",
    "\n",
    "assert os.path.exists(merged_path), f\"Missing {merged_path}\"\n",
    "\n",
    "# load full merged dataset\n",
    "full = pd.read_parquet(merged_path)\n",
    "print(\"Loaded full:\", full.shape)\n",
    "\n",
    "# create dt if missing (safe guard)\n",
    "if 'dt' not in full.columns:\n",
    "    if 'TransactionDT' in full.columns:\n",
    "        START_DATE = \"2017-12-01\"\n",
    "        full['dt'] = pd.to_datetime(full['TransactionDT'], unit='s', origin=START_DATE)\n",
    "    elif 'timestamp' in full.columns:\n",
    "        full['dt'] = pd.to_datetime(full['timestamp'])\n",
    "    else:\n",
    "        raise RuntimeError(\"No dt/TransactionDT/timestamp in merged file.\")\n",
    "\n",
    "# ensure TransactionAmt exists and numeric\n",
    "full['TransactionAmt'] = full['TransactionAmt'].astype(float)\n",
    "\n",
    "# helper functions\n",
    "def rolling_count_seconds(ts_arr, window_seconds):\n",
    "    left_idx = np.searchsorted(ts_arr, ts_arr - window_seconds, side='left')\n",
    "    return (np.arange(len(ts_arr)) - left_idx + 1).astype(np.int32)\n",
    "\n",
    "def rolling_sum_seconds(ts_arr, vals_arr, window_seconds):\n",
    "    left_idx = np.searchsorted(ts_arr, ts_arr - window_seconds, side='left')\n",
    "    csum = np.cumsum(vals_arr)\n",
    "    prev = np.where(left_idx > 0, csum[left_idx - 1], 0.0)\n",
    "    return (csum - prev).astype(float)\n",
    "\n",
    "# prepare seconds column\n",
    "full = full.sort_values(['card1','dt']).reset_index(drop=True)\n",
    "full['ts'] = (full['dt'].astype('int64') // 10**9).astype(np.int64)\n",
    "\n",
    "# windows\n",
    "w_1d = 24*3600; w_7d = 7*24*3600; w_5m = 5*60; w_30m = 30*60\n",
    "\n",
    "# compute card1-level velocity & sums — group_keys=False keeps original index alignment\n",
    "full['card1_txn_count_1d'] = full.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, w_1d), index=g.index)\n",
    ")\n",
    "full['card1_txn_count_7d'] = full.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, w_7d), index=g.index)\n",
    ")\n",
    "full['card1_txn_5min'] = full.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, w_5m), index=g.index)\n",
    ")\n",
    "full['card1_txn_30min'] = full.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, w_30m), index=g.index)\n",
    ")\n",
    "\n",
    "full['card1_amt_sum_1d'] = full.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_sum_seconds(g['ts'].values, g['TransactionAmt'].values, w_1d), index=g.index)\n",
    ")\n",
    "full['card1_amt_sum_7d'] = full.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_sum_seconds(g['ts'].values, g['TransactionAmt'].values, w_7d), index=g.index)\n",
    ")\n",
    "\n",
    "# expanding mean and deviation per card1 (expanding keeps one-to-one alignment)\n",
    "full['card1_amt_mean'] = full.groupby('card1')['TransactionAmt'].expanding().mean().reset_index(level=0, drop=True)\n",
    "full['card1_amt_dev'] = full['TransactionAmt'] - full['card1_amt_mean']\n",
    "\n",
    "# user_region features\n",
    "full['user_region'] = full['card1'].astype(str) + '_' + full['addr1'].astype(str)\n",
    "full = full.sort_values(['user_region','dt']).reset_index(drop=True)\n",
    "full['ts'] = (full['dt'].astype('int64') // 10**9).astype(np.int64)  # recompute ts after resort\n",
    "full['region_txn_count_7d'] = full.groupby('user_region', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, w_7d), index=g.index)\n",
    ")\n",
    "full['region_amt_sum_7d'] = full.groupby('user_region', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_sum_seconds(g['ts'].values, g['TransactionAmt'].values, w_7d), index=g.index)\n",
    ")\n",
    "full['region_amt_mean'] = full.groupby('user_region')['TransactionAmt'].expanding().mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# device features: sort by DeviceInfo\n",
    "full = full.sort_values(['DeviceInfo','dt']).reset_index(drop=True)\n",
    "full['ts'] = (full['dt'].astype('int64') // 10**9).astype(np.int64)\n",
    "full['device_txn_count_7d'] = full.groupby('DeviceInfo', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, w_7d), index=g.index)\n",
    ")\n",
    "full['device_amt_sum_7d'] = full.groupby('DeviceInfo', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_sum_seconds(g['ts'].values, g['TransactionAmt'].values, w_7d), index=g.index)\n",
    ")\n",
    "full['device_amt_mean'] = full.groupby('DeviceInfo')['TransactionAmt'].expanding().mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# sequence features: time since prev, amount diff/ratio\n",
    "full = full.sort_values(['card1','dt']).reset_index(drop=True)\n",
    "full['dt_prev'] = full.groupby('card1')['dt'].shift(1)\n",
    "full['time_since_prev'] = (full['dt'] - full['dt_prev']).dt.total_seconds().fillna(0)\n",
    "full['amt_prev'] = full.groupby('card1')['TransactionAmt'].shift(1).fillna(0)\n",
    "full['amt_diff_prev'] = full['TransactionAmt'] - full['amt_prev']\n",
    "full['amt_ratio_prev'] = full['TransactionAmt'] / (full['amt_prev'] + 1)\n",
    "\n",
    "# final: restore time order and save only feature cols + isFraud + dt\n",
    "full = full.sort_values('dt').reset_index(drop=True)\n",
    "save_cols = [\n",
    "    'card1_txn_count_1d','card1_txn_count_7d','card1_amt_sum_1d','card1_amt_mean','card1_amt_dev',\n",
    "    'region_txn_count_7d','region_amt_mean','device_txn_count_7d','device_amt_mean',\n",
    "    'time_since_prev','amt_diff_prev','amt_ratio_prev','card1_txn_5min','card1_txn_30min',\n",
    "    'isFraud','dt'\n",
    "]\n",
    "# keep only those that exist to avoid errors\n",
    "save_cols = [c for c in save_cols if c in full.columns]\n",
    "full[save_cols].to_parquet(out_feat_path, index=False)\n",
    "\n",
    "print(\"Recomputed & saved features rows:\", len(full), \"cols:\", len(save_cols))\n",
    "print(\"Saved to:\", out_feat_path)\n"
   ],
   "id": "df5c0c91296d1c1e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
