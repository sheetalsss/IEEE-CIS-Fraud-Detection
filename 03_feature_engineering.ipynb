{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-29T01:28:33.792041Z",
     "start_time": "2025-11-29T01:28:32.308494Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# loading and sorting the parquet file\n",
    "train = pd.read_parquet('data/processed/train_merged.parquet')\n",
    "train = train.sort_values(by='dt').reset_index(drop=True)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T01:30:00.391340Z",
     "start_time": "2025-11-29T01:30:00.389304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Keeping the base cols\n",
    "\n",
    "base_cols = [\n",
    "    'TransactionAmt', 'TransactionDT', 'dt',\n",
    "    'card1','addr1','DeviceInfo'\n",
    "]"
   ],
   "id": "22e5c46e80cd23c0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T01:45:54.553218Z",
     "start_time": "2025-11-29T01:45:52.901592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# user level, rolling and aggregated features\n",
    "# `card1`\n",
    "\n",
    "card1_roll = (\n",
    "    train\n",
    "    .set_index('dt')                                # dt becomes index for rolling\n",
    "    .groupby('card1')['TransactionAmt']             # group rows by card1\n",
    "    .rolling('1D')                                  # 1-day rolling\n",
    "    .count()                                        # count inside window\n",
    "    .reset_index()                                  # reset multiindex\n",
    ")\n",
    "\n",
    "card1_roll = card1_roll.rename(columns={'TransactionAmt':'card1_txn_count_1d'})\n",
    "train = train.merge(card1_roll, on=['card1','dt'], how='left')\n",
    "\n",
    "\n",
    "# # counting the no. of transaction for Day 1\n",
    "# train['card1_txn_count_1d'] = (\n",
    "#     train.groupby('card1').rolling('1D', on='dt')['TransactionAmt'].count().reset_index(level=0, drop=True)\n",
    "# )\n",
    "\n",
    "# counting the no. of transaction for Day 7\n",
    "train['card1_txn_count_7d'] = (\n",
    "    train.groupby('card1').\n",
    "    rolling('7D', on='dt')['TransactionAmt'].\n",
    "    count().\n",
    "    reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# Sum of amount per user\n",
    "train['card1_amt_sum_1d'] = (\n",
    "    train.groupby('card1').\n",
    "    rolling('1D', on='dt')['TransactionAmt'].\n",
    "    sum().\n",
    "    reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# user average amount\n",
    "train['card1_amt_mean'] = (\n",
    "    train.groupby('card1')['TransactionAmt']\n",
    "         .expanding()\n",
    "         .mean()\n",
    "         .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# deviation from mean\n",
    "train['card1_amt_dev'] = train['TransactionAmt'] - train['card1_amt_mean']"
   ],
   "id": "c08bbe31a1031283",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reindex on an axis with duplicate labels",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 23\u001B[39m\n\u001B[32m     14\u001B[39m train = train.merge(card1_roll, on=[\u001B[33m'\u001B[39m\u001B[33mcard1\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mdt\u001B[39m\u001B[33m'\u001B[39m], how=\u001B[33m'\u001B[39m\u001B[33mleft\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     17\u001B[39m \u001B[38;5;66;03m# # counting the no. of transaction for Day 1\u001B[39;00m\n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m# train['card1_txn_count_1d'] = (\u001B[39;00m\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m#     train.groupby('card1').rolling('1D', on='dt')['TransactionAmt'].count().reset_index(level=0, drop=True)\u001B[39;00m\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[32m     21\u001B[39m \n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# counting the no. of transaction for Day 7\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mcard1_txn_count_7d\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m = (\n\u001B[32m     24\u001B[39m     train.groupby(\u001B[33m'\u001B[39m\u001B[33mcard1\u001B[39m\u001B[33m'\u001B[39m).\n\u001B[32m     25\u001B[39m     rolling(\u001B[33m'\u001B[39m\u001B[33m7D\u001B[39m\u001B[33m'\u001B[39m, on=\u001B[33m'\u001B[39m\u001B[33mdt\u001B[39m\u001B[33m'\u001B[39m)[\u001B[33m'\u001B[39m\u001B[33mTransactionAmt\u001B[39m\u001B[33m'\u001B[39m].\n\u001B[32m     26\u001B[39m     count().\n\u001B[32m     27\u001B[39m     reset_index(level=\u001B[32m0\u001B[39m, drop=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     28\u001B[39m )\n\u001B[32m     30\u001B[39m \u001B[38;5;66;03m# Sum of amount per user\u001B[39;00m\n\u001B[32m     31\u001B[39m train[\u001B[33m'\u001B[39m\u001B[33mcard1_amt_sum_1d\u001B[39m\u001B[33m'\u001B[39m] = (\n\u001B[32m     32\u001B[39m     train.groupby(\u001B[33m'\u001B[39m\u001B[33mcard1\u001B[39m\u001B[33m'\u001B[39m).\n\u001B[32m     33\u001B[39m     rolling(\u001B[33m'\u001B[39m\u001B[33m1D\u001B[39m\u001B[33m'\u001B[39m, on=\u001B[33m'\u001B[39m\u001B[33mdt\u001B[39m\u001B[33m'\u001B[39m)[\u001B[33m'\u001B[39m\u001B[33mTransactionAmt\u001B[39m\u001B[33m'\u001B[39m].\n\u001B[32m     34\u001B[39m     \u001B[38;5;28msum\u001B[39m().\n\u001B[32m     35\u001B[39m     reset_index(drop=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     36\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:4316\u001B[39m, in \u001B[36mDataFrame.__setitem__\u001B[39m\u001B[34m(self, key, value)\u001B[39m\n\u001B[32m   4313\u001B[39m     \u001B[38;5;28mself\u001B[39m._setitem_array([key], value)\n\u001B[32m   4314\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   4315\u001B[39m     \u001B[38;5;66;03m# set column\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m4316\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_set_item\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:4529\u001B[39m, in \u001B[36mDataFrame._set_item\u001B[39m\u001B[34m(self, key, value)\u001B[39m\n\u001B[32m   4519\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_set_item\u001B[39m(\u001B[38;5;28mself\u001B[39m, key, value) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   4520\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   4521\u001B[39m \u001B[33;03m    Add series to DataFrame in specified column.\u001B[39;00m\n\u001B[32m   4522\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   4527\u001B[39m \u001B[33;03m    ensure homogeneity.\u001B[39;00m\n\u001B[32m   4528\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m4529\u001B[39m     value, refs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sanitize_column\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4531\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   4532\u001B[39m         key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.columns\n\u001B[32m   4533\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m value.ndim == \u001B[32m1\u001B[39m\n\u001B[32m   4534\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value.dtype, ExtensionDtype)\n\u001B[32m   4535\u001B[39m     ):\n\u001B[32m   4536\u001B[39m         \u001B[38;5;66;03m# broadcast across multiple columns if necessary\u001B[39;00m\n\u001B[32m   4537\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.columns.is_unique \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m.columns, MultiIndex):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:5270\u001B[39m, in \u001B[36mDataFrame._sanitize_column\u001B[39m\u001B[34m(self, value)\u001B[39m\n\u001B[32m   5268\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, Series):\n\u001B[32m   5269\u001B[39m         value = Series(value)\n\u001B[32m-> \u001B[39m\u001B[32m5270\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_reindex_for_setitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   5272\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_list_like(value):\n\u001B[32m   5273\u001B[39m     com.require_length_match(value, \u001B[38;5;28mself\u001B[39m.index)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:12699\u001B[39m, in \u001B[36m_reindex_for_setitem\u001B[39m\u001B[34m(value, index)\u001B[39m\n\u001B[32m  12695\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m  12696\u001B[39m     \u001B[38;5;66;03m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001B[39;00m\n\u001B[32m  12697\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m value.index.is_unique:\n\u001B[32m  12698\u001B[39m         \u001B[38;5;66;03m# duplicate axis\u001B[39;00m\n\u001B[32m> \u001B[39m\u001B[32m12699\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[32m  12701\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[32m  12702\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mincompatible index of inserted column with frame index\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m  12703\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n\u001B[32m  12704\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m reindexed_value, \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:12694\u001B[39m, in \u001B[36m_reindex_for_setitem\u001B[39m\u001B[34m(value, index)\u001B[39m\n\u001B[32m  12692\u001B[39m \u001B[38;5;66;03m# GH#4107\u001B[39;00m\n\u001B[32m  12693\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m> \u001B[39m\u001B[32m12694\u001B[39m     reindexed_value = \u001B[43mvalue\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m._values\n\u001B[32m  12695\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m  12696\u001B[39m     \u001B[38;5;66;03m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001B[39;00m\n\u001B[32m  12697\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m value.index.is_unique:\n\u001B[32m  12698\u001B[39m         \u001B[38;5;66;03m# duplicate axis\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/series.py:5164\u001B[39m, in \u001B[36mSeries.reindex\u001B[39m\u001B[34m(self, index, axis, method, copy, level, fill_value, limit, tolerance)\u001B[39m\n\u001B[32m   5147\u001B[39m \u001B[38;5;129m@doc\u001B[39m(\n\u001B[32m   5148\u001B[39m     NDFrame.reindex,  \u001B[38;5;66;03m# type: ignore[has-type]\u001B[39;00m\n\u001B[32m   5149\u001B[39m     klass=_shared_doc_kwargs[\u001B[33m\"\u001B[39m\u001B[33mklass\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   (...)\u001B[39m\u001B[32m   5162\u001B[39m     tolerance=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   5163\u001B[39m ) -> Series:\n\u001B[32m-> \u001B[39m\u001B[32m5164\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreindex\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   5165\u001B[39m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5166\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5167\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5168\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5169\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5170\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlimit\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5171\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtolerance\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtolerance\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5172\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/generic.py:5629\u001B[39m, in \u001B[36mNDFrame.reindex\u001B[39m\u001B[34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001B[39m\n\u001B[32m   5626\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._reindex_multi(axes, copy, fill_value)\n\u001B[32m   5628\u001B[39m \u001B[38;5;66;03m# perform the reindex on the axes\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m5629\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_reindex_axes\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   5630\u001B[39m \u001B[43m    \u001B[49m\u001B[43maxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtolerance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\n\u001B[32m   5631\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m.__finalize__(\u001B[38;5;28mself\u001B[39m, method=\u001B[33m\"\u001B[39m\u001B[33mreindex\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/generic.py:5652\u001B[39m, in \u001B[36mNDFrame._reindex_axes\u001B[39m\u001B[34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001B[39m\n\u001B[32m   5649\u001B[39m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m   5651\u001B[39m ax = \u001B[38;5;28mself\u001B[39m._get_axis(a)\n\u001B[32m-> \u001B[39m\u001B[32m5652\u001B[39m new_index, indexer = \u001B[43max\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreindex\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   5653\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlimit\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtolerance\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtolerance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmethod\u001B[49m\n\u001B[32m   5654\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   5656\u001B[39m axis = \u001B[38;5;28mself\u001B[39m._get_axis_number(a)\n\u001B[32m   5657\u001B[39m obj = obj._reindex_with_indexers(\n\u001B[32m   5658\u001B[39m     {axis: [new_index, indexer]},\n\u001B[32m   5659\u001B[39m     fill_value=fill_value,\n\u001B[32m   5660\u001B[39m     copy=copy,\n\u001B[32m   5661\u001B[39m     allow_dups=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   5662\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:4436\u001B[39m, in \u001B[36mIndex.reindex\u001B[39m\u001B[34m(self, target, method, level, limit, tolerance)\u001B[39m\n\u001B[32m   4433\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mcannot handle a non-unique multi-index!\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   4434\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.is_unique:\n\u001B[32m   4435\u001B[39m     \u001B[38;5;66;03m# GH#42568\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m4436\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mcannot reindex on an axis with duplicate labels\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   4437\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   4438\u001B[39m     indexer, _ = \u001B[38;5;28mself\u001B[39m.get_indexer_non_unique(target)\n",
      "\u001B[31mValueError\u001B[39m: cannot reindex on an axis with duplicate labels"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T03:01:25.938998Z",
     "start_time": "2025-11-29T03:01:18.522040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) ensure correct sort and simple integer index\n",
    "train = train.sort_values(['card1', 'dt']).reset_index(drop=True)\n",
    "\n",
    "# create integer seconds timestamp for fast arithmetic\n",
    "train['ts'] = (train['dt'].astype('int64') // 10**9).astype(np.int64)\n",
    "# ensure TransactionAmt numeric\n",
    "train['TransactionAmt'] = train['TransactionAmt'].astype(float)\n",
    "\n",
    "# helper functions using searchsorted\n",
    "def rolling_count_seconds(ts_arr, window_seconds):\n",
    "    # ts_arr must be 1-D np array of ascending timestamps (seconds)\n",
    "    left_idx = np.searchsorted(ts_arr, ts_arr - window_seconds, side='left')\n",
    "    # counts = current_position_index - left_idx + 1\n",
    "    return (np.arange(len(ts_arr)) - left_idx + 1).astype(np.int32)\n",
    "\n",
    "def rolling_sum_seconds(ts_arr, vals_arr, window_seconds):\n",
    "    left_idx = np.searchsorted(ts_arr, ts_arr - window_seconds, side='left')\n",
    "    csum = np.cumsum(vals_arr)\n",
    "    # sum_i = csum[i] - csum[left_idx[i]-1] (if left_idx>0), else csum[i]\n",
    "    prev_csum = np.where(left_idx > 0, csum[left_idx - 1], 0.0)\n",
    "    return (csum - prev_csum).astype(float)\n",
    "\n",
    "# 2) apply per-group and align back to original index\n",
    "# 1-day (24h) window -> 24*3600 seconds\n",
    "window_1d = 24 * 3600\n",
    "window_7d = 7 * 24 * 3600\n",
    "window_5min = 5 * 60\n",
    "window_30min = 30 * 60\n",
    "\n",
    "# group_keys=False keeps original index inside apply so returned series aligns\n",
    "train['card1_txn_count_1d'] = train.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, window_1d), index=g.index)\n",
    ")\n",
    "train['card1_txn_count_7d'] = train.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, window_7d), index=g.index)\n",
    ")\n",
    "\n",
    "# Velocity Features (fast fraud bursts)\n",
    "train['card1_txn_5min'] = train.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, window_5min), index=g.index)\n",
    ")\n",
    "train['card1_txn_30min'] = train.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, window_30min), index=g.index)\n",
    ")\n",
    "\n",
    "# sums\n",
    "train['card1_amt_sum_1d'] = train.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_sum_seconds(g['ts'].values, g['TransactionAmt'].values, window_1d), index=g.index)\n",
    ")\n",
    "train['card1_amt_sum_7d'] = train.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_sum_seconds(g['ts'].values, g['TransactionAmt'].values, window_7d), index=g.index)\n",
    ")\n"
   ],
   "id": "fb68eefeba5d63fe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_77095/3937829596.py:33: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['card1_txn_count_1d'] = train.groupby('card1', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_77095/3937829596.py:36: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['card1_txn_count_7d'] = train.groupby('card1', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_77095/3937829596.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['card1_txn_5min'] = train.groupby('card1', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_77095/3937829596.py:43: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['card1_txn_30min'] = train.groupby('card1', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_77095/3937829596.py:48: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['card1_amt_sum_1d'] = train.groupby('card1', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_77095/3937829596.py:51: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['card1_amt_sum_7d'] = train.groupby('card1', group_keys=False).apply(\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T06:09:12.304537Z",
     "start_time": "2025-12-01T06:09:11.292350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train = train.sort_values(['card1','dt']).reset_index(drop=True)\n",
    "\n",
    "train['ts'] = (train['dt'].astype('int64') // 10**9).astype('int64')\n",
    "train['TransactionAmt'] = train['TransactionAmt'].astype(float)\n",
    "\n",
    "def rolling_count_seconds(ts_arr, window_seconds):\n",
    "    left_idx = np.searchsorted(ts_arr, ts_arr - window_seconds, side='left')\n",
    "    return np.array(len(ts_arr) - left_idx + 1).astype(np.int32)\n",
    "\n",
    "def rolling_sum_seconds(ts_arr, vals_arr, window_seconds):\n",
    "    left_idx = np.searchsorted(ts_arr, ts_arr - window_seconds, side='left')\n",
    "    csum = np.cumsum(vals_arr)\n",
    "    prev = np.where(left_idx > 0, csum[left_idx - 1], 0.0)\n",
    "    return (csum - prev).astype(float)\n",
    "\n",
    "# 4. apply per-group (1 day, 7 day, 5min, 30min counts and 1d/7d sums)\n",
    "w_1d = 24*3600; w_7d = 7*24*3600; w_5m = 5*60; w_30m = 30*60"
   ],
   "id": "cd2c90d30135ded4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T06:12:23.840649Z",
     "start_time": "2025-12-01T06:12:16.715592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# B) USER+REGION (card1 + addr1)\n",
    "\n",
    "# create user_region key (string is fine)\n",
    "train['user_region'] = train['card1'].astype(str) + '_' + train['addr1'].astype(str)\n",
    "\n",
    "# sort by new group then dt to be safe (groupby.apply will keep original index alignment)\n",
    "train = train.sort_values(['user_region','dt']).reset_index(drop=True)\n",
    "train['ts'] = (train['dt'].astype('int64') // 10**9).astype(np.int64)  # recompute because reset_index\n",
    "\n",
    "# counts and sums per user_region\n",
    "train['region_txn_count_7d'] = train.groupby('user_region', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, w_7d), index=g.index)\n",
    ")\n",
    "train['region_amt_sum_7d'] = train.groupby('user_region', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_sum_seconds(g['ts'].values, g['TransactionAmt'].values, w_7d), index=g.index)\n",
    ")\n"
   ],
   "id": "d8a70ec8cf0d9f9c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_77095/1049355512.py:11: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['region_txn_count_7d'] = train.groupby('user_region', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_77095/1049355512.py:14: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['region_amt_sum_7d'] = train.groupby('user_region', group_keys=False).apply(\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T06:12:37.778505Z",
     "start_time": "2025-12-01T06:12:36.280392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# C) DEVICE (DeviceInfo)\n",
    "# -------------------------\n",
    "train = train.sort_values(['DeviceInfo','dt']).reset_index(drop=True)\n",
    "train['ts'] = (train['dt'].astype('int64') // 10**9).astype(np.int64)\n",
    "\n",
    "train['device_txn_count_7d'] = train.groupby('DeviceInfo', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, w_7d), index=g.index)\n",
    ")\n",
    "train['device_amt_sum_7d'] = train.groupby('DeviceInfo', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_sum_seconds(g['ts'].values, g['TransactionAmt'].values, w_7d), index=g.index)\n",
    ")"
   ],
   "id": "a1c6c712396e7aef",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_77095/359563933.py:6: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  train['device_txn_count_7d'] = train.groupby('DeviceInfo', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_77095/359563933.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['device_txn_count_7d'] = train.groupby('DeviceInfo', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_77095/359563933.py:9: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  train['device_amt_sum_7d'] = train.groupby('DeviceInfo', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_77095/359563933.py:9: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['device_amt_sum_7d'] = train.groupby('DeviceInfo', group_keys=False).apply(\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T06:12:47.809693Z",
     "start_time": "2025-12-01T06:12:47.242944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# final: sort back to original time order (optional)\n",
    "train = train.sort_values('dt').reset_index(drop=True)\n",
    "\n",
    "# quick sanity check print\n",
    "print(train[['card1','dt','TransactionAmt','card1_txn_count_1d','card1_amt_sum_1d']].head(10))\n",
    "print(train[['user_region','region_txn_count_7d','region_amt_sum_7d']].dropna().head(6))\n",
    "print(train[['DeviceInfo','device_txn_count_7d','device_amt_sum_7d']].dropna().head(6))"
   ],
   "id": "85797a50f88476ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   card1                  dt  TransactionAmt  card1_txn_count_1d  \\\n",
      "0  13926 2017-12-02 00:00:00            68.5                   1   \n",
      "1   2755 2017-12-02 00:00:01            29.0                   1   \n",
      "2   4663 2017-12-02 00:01:09            59.0                   1   \n",
      "3  18132 2017-12-02 00:01:39            50.0                   1   \n",
      "4   4497 2017-12-02 00:01:46            50.0                   1   \n",
      "5   5937 2017-12-02 00:01:50            49.0                   1   \n",
      "6  12308 2017-12-02 00:02:02           159.0                   1   \n",
      "7  12695 2017-12-02 00:02:09           422.5                   1   \n",
      "8   2803 2017-12-02 00:02:15            15.0                   1   \n",
      "9  17399 2017-12-02 00:02:16           117.0                   1   \n",
      "\n",
      "   card1_amt_sum_1d  \n",
      "0              68.5  \n",
      "1              29.0  \n",
      "2              59.0  \n",
      "3              50.0  \n",
      "4              50.0  \n",
      "5              49.0  \n",
      "6             159.0  \n",
      "7             422.5  \n",
      "8              15.0  \n",
      "9             117.0  \n",
      "   user_region  region_txn_count_7d  region_amt_sum_7d\n",
      "0  13926_315.0                    4               68.5\n",
      "1   2755_325.0                   62               29.0\n",
      "2   4663_330.0                   35               59.0\n",
      "3  18132_476.0                  258               50.0\n",
      "4   4497_420.0                    2               50.0\n",
      "5   5937_272.0                    6               49.0\n",
      "                       DeviceInfo  device_txn_count_7d  device_amt_sum_7d\n",
      "4   SAMSUNG SM-G892A Build/NRD90M                 10.0          50.000000\n",
      "8                      iOS Device              19786.0          15.000000\n",
      "10                        Windows              47741.0          75.887001\n",
      "16                          MacOS              12574.0          30.000000\n",
      "17                        Windows              47741.0         175.887001\n",
      "40                        Windows              47741.0         251.774002\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T06:19:32.575521Z",
     "start_time": "2025-12-01T06:19:32.539700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sequence features\n",
    "\n",
    "# A.Time since previous transaction\n",
    "train['dt_prev'] = train.groupby('card1')['dt'].shift(1)\n",
    "train['time_since_prev'] = (train['dt'] - train['dt_prev']).dt.total_seconds()\n",
    "\n",
    "# B.Amount difference from previous\n",
    "train['amt_prev'] = train.groupby('card1')['TransactionAmt'].shift(1)\n",
    "train['amt_diff_prev'] = train['TransactionAmt'] - train['amt_prev']\n",
    "\n",
    "# C.Amount ratio\n",
    "train['amt_ratio_prev'] = train['TransactionAmt'] / (train['amt_prev'] + 1)\n"
   ],
   "id": "743dc588aa6192d9",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T06:28:14.562490Z",
     "start_time": "2025-12-01T06:28:11.464782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 0. safety: ensure sorted & sane index\n",
    "train = train.sort_values(['card1','dt']).reset_index(drop=True)\n",
    "\n",
    "# 1. basic ts and numeric type (if not already)\n",
    "train['ts'] = (train['dt'].astype('int64') // 10**9).astype('int64')\n",
    "train['TransactionAmt'] = train['TransactionAmt'].astype(float)\n",
    "\n",
    "# 2. cumulative / expanding mean per card1 (card1_amt_mean) and deviation\n",
    "train['card1_amt_mean'] = (\n",
    "    train.groupby('card1')['TransactionAmt']\n",
    "         .expanding()\n",
    "         .mean()\n",
    "         .reset_index(level=0, drop=True)\n",
    ")\n",
    "train['card1_amt_dev'] = train['TransactionAmt'] - train['card1_amt_mean']\n",
    "\n",
    "# 3. region (card1 + addr1) mean: ensure user_region exists\n",
    "if 'user_region' not in train.columns:\n",
    "    train['user_region'] = train['card1'].astype(str) + '_' + train['addr1'].astype(str)\n",
    "\n",
    "train = train.sort_values(['user_region','dt']).reset_index(drop=True)\n",
    "train['region_amt_mean'] = (\n",
    "    train.groupby('user_region')['TransactionAmt']\n",
    "         .expanding()\n",
    "         .mean()\n",
    "         .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# 4. device mean\n",
    "train = train.sort_values(['DeviceInfo','dt']).reset_index(drop=True)\n",
    "train['device_amt_mean'] = (\n",
    "    train.groupby('DeviceInfo')['TransactionAmt']\n",
    "         .expanding()\n",
    "         .mean()\n",
    "         .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# 5. optional: restore time-order index if you prefer\n",
    "train = train.sort_values('dt').reset_index(drop=True)\n",
    "\n",
    "# 6. sanity: check the new columns exist and show head\n",
    "print(train[['card1_amt_mean','card1_amt_dev','region_amt_mean','device_amt_mean']].head(8))\n",
    "\n",
    "feature_cols = [\n",
    "    'card1_txn_count_1d','card1_txn_count_7d',\n",
    "    'card1_amt_sum_1d','card1_amt_mean','card1_amt_dev',\n",
    "    'region_txn_count_7d','region_amt_mean',\n",
    "    'device_txn_count_7d','device_amt_mean',\n",
    "    'time_since_prev','amt_diff_prev','amt_ratio_prev',\n",
    "    'card1_txn_5min','card1_txn_30min'\n",
    "]\n",
    "\n",
    "# 7. now save the features (use only columns that exist)\n",
    "available = [c for c in feature_cols + ['isFraud'] if c in train.columns]\n",
    "train[available].to_parquet(\"data/processed/train_features_day3.parquet\", index=False)\n",
    "print(\"Saved parquet with columns:\", available)\n"
   ],
   "id": "28461aba019b6555",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_77095/3729328270.py:32: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  train.groupby('DeviceInfo')['TransactionAmt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   card1_amt_mean  card1_amt_dev  region_amt_mean  device_amt_mean\n",
      "0            68.5            0.0             68.5              NaN\n",
      "1            29.0            0.0             29.0              NaN\n",
      "2            59.0            0.0             59.0              NaN\n",
      "3            50.0            0.0             50.0              NaN\n",
      "4            50.0            0.0             50.0             50.0\n",
      "5            49.0            0.0             49.0              NaN\n",
      "6           159.0            0.0            159.0              NaN\n",
      "7           422.5            0.0            422.5              NaN\n",
      "Saved parquet with columns: ['card1_txn_count_1d', 'card1_txn_count_7d', 'card1_amt_sum_1d', 'card1_amt_mean', 'card1_amt_dev', 'region_txn_count_7d', 'region_amt_mean', 'device_txn_count_7d', 'device_amt_mean', 'time_since_prev', 'amt_diff_prev', 'amt_ratio_prev', 'card1_txn_5min', 'card1_txn_30min', 'isFraud']\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cb4cd2c8ae218d54"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
