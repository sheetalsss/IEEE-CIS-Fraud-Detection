{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T03:34:37.933047Z",
     "start_time": "2025-12-04T03:34:37.359405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_parquet('data/processed/train_full.parquet')\n",
    "train.shape"
   ],
   "id": "ccaf0fd0556ca536",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 443)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-04T03:42:13.710350Z",
     "start_time": "2025-12-04T03:42:13.652433Z"
    }
   },
   "source": [
    "# Frequency Encoding for Key High-Cardinality Columns\n",
    "\n",
    "cols = ['card1', 'card2', 'card3', 'DeviceInfo', 'addr1', 'P_emaildomain', 'R_emaildomain', 'id_17', 'id_31']\n",
    "new_cols = []\n",
    "for col in cols:\n",
    "    freq = train[col].value_counts()\n",
    "    new_col = f'{col}_freq'\n",
    "    new_cols.append(new_col)\n",
    "    train[new_col] = train[col].map(freq)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T03:50:34.370305Z",
     "start_time": "2025-12-04T03:50:34.343503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# time based features\n",
    "\n",
    "train['hour'] = train['dt'].dt.hour\n",
    "train['weekday'] = train['dt'].dt.weekday\n",
    "train['day'] = train['dt'].dt.day\n",
    "\n",
    "train['is_weekend'] = (train['weekday'] > 5).astype(int)\n",
    "train['is_night'] = ((train['hour']) >=0 & (train['hour'] <= 6)).astype(int)"
   ],
   "id": "72ace8c14019186c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T12:45:14.327592Z",
     "start_time": "2025-12-04T12:45:13.091439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Account-Age Features\n",
    "# Why this matters?\n",
    "# Open an account/card/device and immediately start transacting\n",
    "# Or use a new address/device for a single fraud event\n",
    "# Or show abrupt account creation signals\n",
    "\n",
    "# Real users have:\n",
    "# Long histories\n",
    "# Stable gradual behavior\n",
    "# So “age since first seen” becomes a strong discriminator.\n",
    "\n",
    "train = train.sort_values(['card1','dt']).reset_index(drop=True)\n",
    "\n",
    "if 'ts' not in train.columns:\n",
    "    train['ts'] = train['dt'].astype('int64') // 10**9"
   ],
   "id": "6f786f2f251bcea6",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T12:56:04.500580Z",
     "start_time": "2025-12-04T12:56:04.457901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# card1 Age\n",
    "# calculate the age in seconds and days\n",
    "first_ts_card1 = train.groupby('card1')['ts'].transform('min')\n",
    "train['card1_age_sec'] = train['ts'] - first_ts_card1\n",
    "train['card1_age_days'] = train['card1_age_sec'] / 86400\n",
    "train[['card1','ts','card1_age_sec', 'card1_age_days']].head()\n",
    "\n",
    "mean_age_card1 = train.groupby('card1')['card1_age_sec'].transform('mean')\n",
    "train['card1_age_norm'] = train['card1_age_sec'] / (mean_age_card1 + 1)"
   ],
   "id": "36c636fc23867f51",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T12:57:54.629212Z",
     "start_time": "2025-12-04T12:57:54.610187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# device age\n",
    "\n",
    "first_ts_device = train.groupby('DeviceInfo')['ts'].transform('min')\n",
    "train['device_age_sec'] = train['ts'] - first_ts_device\n",
    "train['device_age_days'] = train['device_age_sec'] / 86400\n",
    "\n",
    "mean_age_dev = train.groupby('DeviceInfo')['device_age_sec'].transform('mean')\n",
    "train['device_age_norm'] = train['device_age_sec'] / (mean_age_dev + 1)"
   ],
   "id": "411b71e1404ab8fe",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T12:59:21.854677Z",
     "start_time": "2025-12-04T12:59:21.831446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# addr1 age\n",
    "\n",
    "first_ts_addr1 = train.groupby('addr1')['ts'].transform('min')\n",
    "train['addr1_age_sec'] = train['ts'] - first_ts_addr1\n",
    "train['addr1_age_days'] = train['addr1_age_sec'] / 86400\n",
    "\n",
    "mean_age_addr1 = train.groupby('addr1')['addr1_age_sec'].transform('mean')\n",
    "train['addr1_age_norm'] = train['addr1_age_sec'] / (mean_age_addr1 + 1)"
   ],
   "id": "ac97bb2c805424d3",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T12:59:35.880122Z",
     "start_time": "2025-12-04T12:59:35.862059Z"
    }
   },
   "cell_type": "code",
   "source": "train[['card1_age_days','card1_age_norm','device_age_days','addr1_age_norm']].head()",
   "id": "7f4963d979cf6489",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   card1_age_days  card1_age_norm  device_age_days  addr1_age_norm\n",
       "0        0.000000        0.000000        43.219757        0.777083\n",
       "1        0.000000        0.000000         9.604954        0.108947\n",
       "2       29.952685        1.118355        39.557639        0.449633\n",
       "3       50.395718        1.881644        60.000671        0.682155\n",
       "4        0.000000        0.000000         8.752326        0.107747"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card1_age_days</th>\n",
       "      <th>card1_age_norm</th>\n",
       "      <th>device_age_days</th>\n",
       "      <th>addr1_age_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.219757</td>\n",
       "      <td>0.777083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.604954</td>\n",
       "      <td>0.108947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.952685</td>\n",
       "      <td>1.118355</td>\n",
       "      <td>39.557639</td>\n",
       "      <td>0.449633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.395718</td>\n",
       "      <td>1.881644</td>\n",
       "      <td>60.000671</td>\n",
       "      <td>0.682155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.752326</td>\n",
       "      <td>0.107747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:08:38.536999Z",
     "start_time": "2025-12-04T13:08:38.285161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalized Amount Features\n",
    "\n",
    "\n",
    "# # card1\n",
    "# card1_amt_mean = train.groupby('card1')['TransactionAmt'].transform('mean')\n",
    "# train['amt_norm_card1'] = train['TransactionAmt'] / (card1_amt_mean + 1)\n",
    "#\n",
    "# # region\n",
    "# train['user_region'] = train['card1'].astype(str)+'_'+train['addr1'].astype(str)\n",
    "# region_amt_mean = train.groupby('user_region')['TransactionAmt'].transform('mean')\n",
    "# train['amt_norm_region'] = train['TransactionAmt'] / (region_amt_mean + 1)\n",
    "#\n",
    "# # device\n",
    "# device_amt_mean = train.groupby('DeviceInfo')['TransactionAmt'].transform('mean')\n",
    "# train['amt_norm_device'] = train['TransactionAmt'] / (device_amt_mean + 1)\n"
   ],
   "id": "854c3571b0a29025",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:11:56.683311Z",
     "start_time": "2025-12-04T13:11:55.180137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalized features\n",
    "\n",
    "# must sort by time first\n",
    "train = train.sort_values(['card1', 'dt'])\n",
    "\n",
    "# card1 expanding mean (causal)\n",
    "train['card1_amt_mean'] = (\n",
    "    train.groupby('card1')['TransactionAmt']\n",
    "         .expanding()\n",
    "         .mean()\n",
    "         .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "train['amt_norm_card1'] = train['TransactionAmt'] / (train['card1_amt_mean'] + 1)\n"
   ],
   "id": "452ed39ecb880724",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:13:51.348798Z",
     "start_time": "2025-12-04T13:13:50.207355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# region\n",
    "train['user_region'] = train['card1'].astype(str)+'_'+train['addr1'].astype(str)\n",
    "train = train.sort_values(['user_region', 'dt'])\n",
    "\n",
    "train['region_amt_mean'] = train.groupby('user_region')['TransactionAmt'].expanding().mean().reset_index(level=0, drop=True)\n"
   ],
   "id": "dc9e624a9211c13d",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:14:19.087268Z",
     "start_time": "2025-12-04T13:14:18.102962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# deviceInfo\n",
    "train = train.sort_values(['DeviceInfo','dt'])\n",
    "\n",
    "train['device_amt_mean'] = (\n",
    "    train.groupby('DeviceInfo')['TransactionAmt']\n",
    "         .expanding()\n",
    "         .mean()\n",
    "         .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "train['amt_norm_device'] = train['TransactionAmt'] / (train['device_amt_mean'] + 1)\n"
   ],
   "id": "2fd9899d4d8839d5",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. z_amt_card1 → How abnormal the transaction amount is compared to the user’s historical mean (higher = more suspicious).\n",
    "2. card1_std_1d → Variation in the user’s transaction amounts in the past 1 day (fraud increases short-term volatility).\n",
    "3. card1_std_7d → Variation in the user’s transaction amounts in the past 7 days (captures multi-day instability).\n",
    "4. card1_min_7d → Smallest transaction amount by this user in the last 7 days (detects “test” micro-transactions).\n",
    "5. card1_max_7d → Largest transaction amount by this user in the last 7 days (captures extreme spikes).\n",
    "6. volatility_7d → Ratio of std to mean over 7 days; measures inconsistent spending behavior (fraud spikes cause high volatility)."
   ],
   "id": "d0450be8a0f4ee8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T14:24:24.691707Z",
     "start_time": "2025-12-04T14:24:15.077402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ensure prerequisites\n",
    "assert 'TransactionAmt' in train.columns and 'card1' in train.columns and 'dt' in train.columns\n",
    "train['TransactionAmt'] =  train['TransactionAmt'].astype(float)\n",
    "train['dt'] = pd.to_datetime(train['dt'])\n",
    "\n",
    "# sort for group ops\n",
    "train = train.sort_values(['card1','dt']).reset_index(drop=True)\n",
    "train['ts'] = (train['dt'].astype('int64') // 10**9).astype(np.int64)\n",
    "\n",
    "# helper numeric rolling functions using searchsorted (fast & index-aligned)\n",
    "def rolling_count_seconds(ts_arr, window_seconds):\n",
    "    left_idx = np.searchsorted(ts_arr, ts_arr - window_seconds, side='left')\n",
    "    return (np.arange(len(ts_arr)) - left_idx + 1).astype(np.int32)\n",
    "\n",
    "def rolling_sum_seconds(ts_arr, vals_arr, window_seconds):\n",
    "    left_idx = np.searchsorted(ts_arr, ts_arr - window_seconds, side='left')\n",
    "    csum = np.cumsum(vals_arr)\n",
    "    prev = np.where(left_idx > 0, csum[left_idx - 1], 0.0)\n",
    "    return (csum - prev).astype(float)\n",
    "\n",
    "def rolling_sumsq_seconds(ts_arr, vals_arr, window_seconds):\n",
    "    vsq = vals_arr.astype(float) ** 2\n",
    "    left_idx = np.searchsorted(ts_arr, ts_arr - window_seconds, side='left')\n",
    "    csum = np.cumsum(vsq)\n",
    "    prev = np.where(left_idx > 0, csum[left_idx - 1], 0.0)\n",
    "    return (csum - prev).astype(float)\n",
    "\n",
    "# windows (seconds)\n",
    "w_1d = 24*3600\n",
    "w_7d = 7*24*3600\n",
    "\n",
    "# --- numeric rolling: count, sum, sumsq -> mean & std ---\n",
    "train['card1_cnt_1d'] = train.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, w_1d), index=g.index)\n",
    ")\n",
    "train['card1_cnt_7d'] = train.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_count_seconds(g['ts'].values, w_7d), index=g.index)\n",
    ")\n",
    "\n",
    "train['card1_sum_1d'] = train.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_sum_seconds(g['ts'].values, g['TransactionAmt'].values, w_1d), index=g.index)\n",
    ")\n",
    "train['card1_sum_7d'] = train.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_sum_seconds(g['ts'].values, g['TransactionAmt'].values, w_7d), index=g.index)\n",
    ")\n",
    "\n",
    "train['card1_sumsq_1d'] = train.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_sumsq_seconds(g['ts'].values, g['TransactionAmt'].values, w_1d), index=g.index)\n",
    ")\n",
    "train['card1_sumsq_7d'] = train.groupby('card1', group_keys=False).apply(\n",
    "    lambda g: pd.Series(rolling_sumsq_seconds(g['ts'].values, g['TransactionAmt'].values, w_7d), index=g.index)\n",
    ")\n",
    "\n",
    "# means\n",
    "train['card1_mean_1d'] = train['card1_sum_1d'] / (train['card1_cnt_1d'] + 1e-9)\n",
    "train['card1_mean_7d'] = train['card1_sum_7d'] / (train['card1_cnt_7d'] + 1e-9)\n",
    "\n",
    "# std from sums and sumsq: var = E[x^2] - (E[x])^2\n",
    "var_1d = (train['card1_sumsq_1d'] / (train['card1_cnt_1d'] + 1e-9)) - (train['card1_mean_1d'] ** 2)\n",
    "var_7d = (train['card1_sumsq_7d'] / (train['card1_cnt_7d'] + 1e-9)) - (train['card1_mean_7d'] ** 2)\n",
    "train['card1_std_1d'] = np.sqrt(np.clip(var_1d, 0, None))\n",
    "train['card1_std_7d'] = np.sqrt(np.clip(var_7d, 0, None))\n",
    "\n",
    "# z-score anomaly: use card1 historical expanding mean/dev if available else window mean/std\n",
    "# Prefer card1_amt_mean/card1_amt_dev if you have them (expanding historical). Otherwise use rolling stats.\n",
    "if 'card1_amt_mean' in train.columns and 'card1_amt_dev' in train.columns:\n",
    "    train['z_amt_card1'] = (train['TransactionAmt'] - train['card1_amt_mean']) / (np.abs(train['card1_amt_dev']) + 1)\n",
    "else:\n",
    "    train['z_amt_card1'] = (train['TransactionAmt'] - train['card1_mean_7d']) / (train['card1_std_7d'] + 1)\n",
    "\n",
    "# volatility: rolling_std_7d / (rolling_mean_7d + 1)\n",
    "train['volatility_7d'] = train['card1_std_7d'] / (train['card1_mean_7d'] + 1)\n",
    "\n",
    "# rolling_min and rolling_max (7d) — use pandas time-rolling per-group then merge safely\n",
    "tmp_min = (\n",
    "    train.set_index('dt')\n",
    "        .groupby('card1')['TransactionAmt']\n",
    "        .rolling('7D')\n",
    "        .min()\n",
    "        .reset_index()\n",
    "        .rename(columns={'TransactionAmt':'card1_min_7d'})\n",
    ")\n",
    "train = train.merge(tmp_min, on=['card1','dt'], how='left')\n",
    "\n",
    "tmp_max = (\n",
    "    train.set_index('dt')\n",
    "        .groupby('card1')['TransactionAmt']\n",
    "        .rolling('7D')\n",
    "        .max()\n",
    "        .reset_index()\n",
    "        .rename(columns={'TransactionAmt':'card1_max_7d'})\n",
    ")\n",
    "train = train.merge(tmp_max, on=['card1','dt'], how='left')\n",
    "\n",
    "# min/max can be NaN for first rows; replace with current TransactionAmt where appropriate\n",
    "train['card1_min_7d'] = train['card1_min_7d'].fillna(train['TransactionAmt'])\n",
    "train['card1_max_7d'] = train['card1_max_7d'].fillna(train['TransactionAmt'])\n",
    "\n",
    "# final cleanup: replace inf/nan if any\n",
    "for c in ['z_amt_card1','card1_std_1d','card1_std_7d','volatility_7d','card1_min_7d','card1_max_7d']:\n",
    "    if c in train.columns:\n",
    "        train[c].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        train[c].fillna(0, inplace=True)\n",
    "\n",
    "# show results\n",
    "print(train[['TransactionAmt','card1_amt_mean','card1_amt_dev','z_amt_card1',\n",
    "            'card1_std_1d','card1_std_7d','card1_min_7d','card1_max_7d','volatility_7d']].head(8))\n"
   ],
   "id": "ad7a7a8cf3ac270d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_81189/1801178011.py:36: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['card1_cnt_1d'] = train.groupby('card1', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_81189/1801178011.py:39: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['card1_cnt_7d'] = train.groupby('card1', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_81189/1801178011.py:43: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['card1_sum_1d'] = train.groupby('card1', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_81189/1801178011.py:46: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['card1_sum_7d'] = train.groupby('card1', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_81189/1801178011.py:50: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['card1_sumsq_1d'] = train.groupby('card1', group_keys=False).apply(\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_81189/1801178011.py:53: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['card1_sumsq_7d'] = train.groupby('card1', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TransactionAmt  card1_amt_mean  card1_amt_dev  z_amt_card1  card1_std_1d  \\\n",
      "0       23.443001       23.443001       0.000000     0.000000      0.000741   \n",
      "1      183.000000      183.000000       0.000000     0.000000      0.005787   \n",
      "2       29.000000      106.000000     -77.000000    -0.987179      0.000917   \n",
      "3       27.000000       79.666667     -52.666667    -0.981366      0.000854   \n",
      "4      150.000000      150.000000       0.000000     0.000000      0.004743   \n",
      "5       30.000000       90.000000     -60.000000    -0.983607      0.000949   \n",
      "6       50.000000       76.666667     -26.666667    -0.963855      0.001581   \n",
      "7      226.000000      114.000000     112.000000     0.991150      0.007147   \n",
      "\n",
      "   card1_std_7d  card1_min_7d  card1_max_7d  volatility_7d  \n",
      "0      0.000741     23.443001     23.443001       0.000030  \n",
      "1      0.005787    183.000000    183.000000       0.000031  \n",
      "2      0.000917     29.000000     29.000000       0.000031  \n",
      "3      0.000854     27.000000     27.000000       0.000030  \n",
      "4      0.004743    150.000000    150.000000       0.000031  \n",
      "5     60.000000     30.000000    150.000000       0.659341  \n",
      "6      0.001581     50.000000     50.000000       0.000031  \n",
      "7      0.007147    226.000000    226.000000       0.000031  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_81189/1801178011.py:105: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train[c].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "/var/folders/0l/p2_v2ssd19s30xk4x88_7cy00000gn/T/ipykernel_81189/1801178011.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train[c].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "These anomaly features quantify how much the current transaction deviates from the user’s normal behavior. Fraud almost always increases:\n",
    "- deviation from mean\n",
    "- volatility\n",
    "- extreme min/max range\n",
    "- instability over time\n",
    "\n",
    "So the model learns patterns like:\n",
    "“If amount is normal → low risk. If amount is a sudden spike or inconsistent with history → higher fraud probability.”"
   ],
   "id": "237c5d3e1e26f3f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:43:12.466709Z",
     "start_time": "2025-12-04T15:43:12.462970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PCA on V-Features (huge boost)\n",
    "# PCA compresses 339 noisy correlated features into 10 clean orthogonal components.\n",
    "\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "id": "9cb66ad5573dfda7",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:43:22.507622Z",
     "start_time": "2025-12-04T15:43:18.801018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = len(train)\n",
    "train_end = int(0.70 * n)\n",
    "\n",
    "V_cols = [c for c in train.columns if c.startswith('V')]\n",
    "V_df = train[V_cols].fillna(train[V_cols].median())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_v = scaler.fit_transform(V_df.iloc[:train_end].values)\n",
    "X_all_v = scaler.transform(V_df.values)\n",
    "\n",
    "n_components = 10\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "pca.fit(X_train_v)\n",
    "X_all_pca = pca.transform(X_all_v)\n",
    "\n",
    "pca_cols = [f\"V_pca_{i+1}\" for i in range(n_components)]\n",
    "pca_df = pd.DataFrame(X_all_pca, columns=pca_cols, index=train.index)\n",
    "\n",
    "# attach safely (preserves 1:1 alignment)\n",
    "for c in pca_cols:\n",
    "    train[c] = pca_df[c].values\n",
    "\n",
    "# save outputs\n",
    "train[pca_cols + ['dt']].to_parquet(\"data/processed/day5_v_pca.parquet\", index=False)\n",
    "joblib.dump(scaler, \"models/v_scaler.joblib\")\n",
    "joblib.dump(pca, \"models/v_pca.joblib\")\n",
    "\n",
    "print(\"Saved PCA, explained var:\", np.round(pca.explained_variance_ratio_,4))"
   ],
   "id": "2f96a77eafca037d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/decomposition/_pca.py:604: RuntimeWarning: divide by zero encountered in matmul\n",
      "  C = X.T @ X\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/decomposition/_pca.py:604: RuntimeWarning: overflow encountered in matmul\n",
      "  C = X.T @ X\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/decomposition/_pca.py:604: RuntimeWarning: invalid value encountered in matmul\n",
      "  C = X.T @ X\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning: divide by zero encountered in matmul\n",
      "  X_transformed = X @ self.components_.T\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning: overflow encountered in matmul\n",
      "  X_transformed = X @ self.components_.T\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning: invalid value encountered in matmul\n",
      "  X_transformed = X @ self.components_.T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PCA, explained var: [0.5258 0.0936 0.0902 0.0478 0.0393 0.0308 0.0233 0.0225 0.0209 0.0131]\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:46:29.787734Z",
     "start_time": "2025-12-04T15:46:22.910162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = len(train)\n",
    "train_end = int(0.70 * n)\n",
    "\n",
    "# ======================\n",
    "# 1. Collect V columns\n",
    "# ======================\n",
    "V_cols = [c for c in train.columns if c.startswith('V')]\n",
    "V_df = train[V_cols].copy()\n",
    "\n",
    "# ======================\n",
    "# 2. Clean V-features\n",
    "# ======================\n",
    "\n",
    "# (a) Replace inf/-inf → NaN\n",
    "V_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# (b) Remove zero-variance columns\n",
    "var = V_df.var()\n",
    "zero_var_cols = var[var == 0].index.tolist()\n",
    "if zero_var_cols:\n",
    "    print(\"Dropping zero-variance V-cols:\", zero_var_cols)\n",
    "    V_df = V_df.drop(columns=zero_var_cols)\n",
    "\n",
    "# (c) Clip extreme outliers (prevents overflow inside PCA)\n",
    "V_df = V_df.clip(-1e6, 1e6)\n",
    "\n",
    "# (d) Fill NaN with column median\n",
    "V_df = V_df.fillna(V_df.median())\n",
    "\n",
    "# ======================\n",
    "# 3. Standardize (fit on train only)\n",
    "# ======================\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_v = scaler.fit_transform(V_df.iloc[:train_end])\n",
    "X_all_v   = scaler.transform(V_df)\n",
    "\n",
    "# ======================\n",
    "# 4. PCA (fit on train only)\n",
    "# ======================\n",
    "n_components = 10\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "\n",
    "pca.fit(X_train_v)\n",
    "X_all_pca = pca.transform(X_all_v)\n",
    "\n",
    "# ======================\n",
    "# 5. Create PCA dataframe\n",
    "# ======================\n",
    "pca_cols = [f\"V_pca_{i+1}\" for i in range(n_components)]\n",
    "pca_df = pd.DataFrame(X_all_pca, columns=pca_cols, index=train.index)\n",
    "\n",
    "# ======================\n",
    "# 6. Attach PCA features back\n",
    "# ======================\n",
    "for c in pca_cols:\n",
    "    train[c] = pca_df[c].values\n",
    "\n",
    "# ======================\n",
    "# 7. Save outputs\n",
    "# ======================\n",
    "train[pca_cols + ['dt']].to_parquet(\"data/processed/day5_v_pca.parquet\", index=False)\n",
    "joblib.dump(scaler, \"models/v_scaler.joblib\")\n",
    "joblib.dump(pca, \"models/v_pca.joblib\")\n",
    "\n",
    "print(\"Saved PCA, explained var:\", np.round(pca.explained_variance_ratio_, 4))\n"
   ],
   "id": "de48bca165df4335",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/decomposition/_pca.py:604: RuntimeWarning: divide by zero encountered in matmul\n",
      "  C = X.T @ X\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/decomposition/_pca.py:604: RuntimeWarning: overflow encountered in matmul\n",
      "  C = X.T @ X\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/decomposition/_pca.py:604: RuntimeWarning: invalid value encountered in matmul\n",
      "  C = X.T @ X\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning: divide by zero encountered in matmul\n",
      "  X_transformed = X @ self.components_.T\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning: overflow encountered in matmul\n",
      "  X_transformed = X @ self.components_.T\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning: invalid value encountered in matmul\n",
      "  X_transformed = X @ self.components_.T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PCA, explained var: [0.5136 0.0938 0.0905 0.0493 0.041  0.0328 0.0255 0.0247 0.0231 0.0156]\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**What I did**\n",
    "* Selected all V-features (V1…V*).\n",
    "* Cleaned them: replaced INF, dropped zero-variance columns, clipped extreme values, filled NaNs.\n",
    "* Standardized V-features (fit on first 70% only → no leakage).\n",
    "* Ran PCA with 10 components fitted on the same 70% slice.\n",
    "* Transformed the full dataset → created V_pca_1 … V_pca_10.\n",
    "* Saved scaler + PCA models for inference.\n",
    "\n",
    "**Why PCA**\n",
    "* V-features are ~340 noisy, correlated variables from a latent risk engine.\n",
    "* PCA compresses them into a few dense, orthogonal signals that generalize better.\n",
    "* Removes noise, reduces redundancy, and improves model stability & AUC.\n",
    "\n",
    "**Key Insight**\n",
    "* PC1 alone explains ~51% of variance → a strong hidden fraud signal.\n",
    "* Top 10 PCs capture ~81% of all V-information with only 10 features.\n",
    "\n",
    "**Impact**\n",
    "* Model gets clearer signals, less noise, faster training.\n",
    "* Expected boost: +0.01–0.02 ROC, better top-1% fraud capture."
   ],
   "id": "c0bcdf5f3b682bbd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T06:35:41.830241Z",
     "start_time": "2025-12-05T06:35:41.821969Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "74cc0f1cf4e1d0b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598298, 495)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bb08c04fd93d6060"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
